{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://shijuan.zww.cn/chuzhong/yingyu/nianji{}/list/\"\n",
    "OUTPUT_DIR = \"exercise{}\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_links(url):\n",
    "    response = requests.get(url, headers=HEADERS, timeout=20)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to access {url}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    article_links = []\n",
    "\n",
    "    for link in soup.select(\"a\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href and \"/chuzhong/yingyu/nianji\" in href:\n",
    "            full_url = \"http://shijuan.zww.cn\" + href if href.startswith(\"/\") else href\n",
    "            if not full_url.endswith(\"list/\"):\n",
    "                article_links.append(full_url)\n",
    "    \n",
    "    return list(set(article_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {(1, 47), (2, 48), (3, 49)} # year, total pages\n",
    "# tasks = {(1, 2), (2, 4), (3, 4)}\n",
    "article_links = {}\n",
    "failed_link = {}\n",
    "\n",
    "for year, total_pages in tasks:\n",
    "    year_link = BASE_URL.format(year)\n",
    "    article_links[year] = get_page_links(year_link)\n",
    "    failed_link[year] = []\n",
    "\n",
    "    for i in tqdm(range(1, total_pages)):\n",
    "        try:\n",
    "            new_part = get_page_links(f\"{year_link}{i}.htm\")\n",
    "            article_links[year].extend(new_part)\n",
    "        except:\n",
    "            print(f\"Failed to get page {i} for year {year}\")\n",
    "            failed_link[year].append(f\"{year_link}{i}.htm\")\n",
    "        # add a random delay to avoid being blocked\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    article_links[year] = list(set(article_links[year]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write article links to txt file\n",
    "for year, links in article_links.items():\n",
    "    with open(f\"article_links_{year}.txt\", \"w\") as f:\n",
    "        for link in links:\n",
    "            f.write(link + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import chardet  # To detect encoding dynamically\n",
    "\n",
    "# Function to detect the correct encoding\n",
    "def detect_encoding(content):\n",
    "    result = chardet.detect(content)\n",
    "    return result[\"encoding\"] if result[\"encoding\"] else \"utf-8\"\n",
    "\n",
    "# Function to clean and normalize text\n",
    "def clean_text(text):\n",
    "    # Ensure the text is valid UTF-8\n",
    "    text = text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n",
    "    \n",
    "    # Replace Chinese punctuation with English equivalents\n",
    "    replacements = {\n",
    "        \"。\": \".\", \"，\": \",\", \"、\": \",\", \"？\": \"?\", \"！\": \"!\", \"：\": \":\", \"；\": \";\",\n",
    "        \"（\": \"(\", \"）\": \")\", \"【\": \"[\", \"】\": \"]\", \"“\": \"\\\"\", \"”\": \"\\\"\", \"‘\": \"'\", \"’\": \"'\",\n",
    "        \"《\": \"<\", \"》\": \">\"\n",
    "    }\n",
    "    for ch, eng in replacements.items():\n",
    "        text = text.replace(ch, eng)\n",
    "\n",
    "    # Replace HTML non-breaking spaces and Chinese full-width space\n",
    "    text = text.replace(\"&nbsp;\", \" \").replace(\"\\u3000\", \" \")\n",
    "\n",
    "    # Remove \"¡¡¡¡\" (encoding artifacts from web scraping)\n",
    "    text = re.sub(r\"¡+\", \" \", text)\n",
    "    \n",
    "    # Ensure no newlines and strip extra spaces\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to extract content and generate JSON output\n",
    "def get_article_data(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Detect encoding dynamically and decode response content\n",
    "    encoding = detect_encoding(response.content)\n",
    "    response.encoding = encoding  # Ensure correct decoding\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch article: {url}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Find the <div> with class 'content' and extract all <p> tags within it\n",
    "    content_div = soup.find(\"div\", class_=\"content\")\n",
    "    \n",
    "    if not content_div:\n",
    "        raise Exception(\"Content not found\")\n",
    "    \n",
    "    paragraphs = content_div.find_all(\"p\")\n",
    "    \n",
    "    # Process each <p> tag\n",
    "    cleaned_paragraphs = []\n",
    "    for p_tag in paragraphs:\n",
    "        # Within each <p>, find all <u> tags\n",
    "        for u_tag in p_tag.find_all('u'):\n",
    "            # Replace <u> tag with its text wrapped in underscores\n",
    "            u_tag.replace_with(f'_{u_tag.get_text()}_')\n",
    "        \n",
    "        # Clean and extract text from the modified <p> tag\n",
    "        cleaned_paragraphs.append(clean_text(p_tag.get_text(strip=True)))\n",
    "    \n",
    "    # Create the final JSON object\n",
    "    result = {\n",
    "        \"url\": url,\n",
    "        \"content\": cleaned_paragraphs\n",
    "    }\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_link = {}\n",
    "\n",
    "for year in range(1, 4):\n",
    "    os.makedirs(f\"zww.cn_nianji{year}\", exist_ok=True)\n",
    "    filtered_list = []\n",
    "    failed_link[year] = []\n",
    "    with open(\"article_links_{}.txt\".format(year), \"r\") as f:\n",
    "        for line in f:\n",
    "            filtered_list.append(line.strip())   \n",
    "\n",
    "    for filtered_link in tqdm(filtered_list):\n",
    "        try:\n",
    "            ret = get_article_data(filtered_link)\n",
    "            filename = filtered_link.split(\"/\")[-1][:-4]\n",
    "            json.dump(ret, open(f\"zww.cn_nianji{year}/{filename}.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=4)\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "        except Exception:\n",
    "            print(f\"Failed to process {filtered_link}\")\n",
    "            failed_link[year].append(filtered_link)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs553",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
