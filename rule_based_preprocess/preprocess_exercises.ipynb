{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import enum\n",
    "\n",
    "\n",
    "class QuestionType(enum.Enum):\n",
    "    MCQ = \"Multiple-Choice-Question\"\n",
    "    MC_READING = \"Reading-Comprehension-With-Multiple-Choices\"\n",
    "    TF_READING = \"Reading-Comprehension-With-True-or-False\"\n",
    "    MC_CLOZE = \"Cloze-With-Multiple-Choices\"\n",
    "    FR_CLOZE = \"Cloze-With-Free-Responses\"\n",
    "    # not to support other types for now\n",
    "\n",
    "class Choices(BaseModel):\n",
    "    A: str\n",
    "    B: str\n",
    "    C: str\n",
    "    D: str\n",
    "\n",
    "class Question(BaseModel):\n",
    "    text: str\n",
    "    choices: Choices\n",
    "    answer: str\n",
    "    is_answer_provided: bool\n",
    "    explanation: str\n",
    "    test_point: str\n",
    "\n",
    "class QuestionSet(BaseModel):\n",
    "    type: QuestionType\n",
    "    context: str\n",
    "    questions: list[Question]\n",
    "\n",
    "class Exam(BaseModel):\n",
    "    question_sets: list[QuestionSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assort exercises as per the categories\n",
    "root_path_raw = \"\"\n",
    "root_path = \"\"\n",
    "output_path = \"\"\n",
    "filtered_output_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set logging level (INFO, ERROR, DEBUG, etc.)\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Add timestamp\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",  # Customize timestamp format\n",
    "    force=True,\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"output.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = os.listdir(root_path_raw)\n",
    "sources = [source for source in sources if os.path.isdir(os.path.join(root_path_raw, source))]\n",
    "if \"filtered\" in sources:\n",
    "    sources.remove(\"filtered\")\n",
    "\n",
    "for source in sources:\n",
    "    for subdir in os.listdir(os.path.join(root_path_raw, source)):\n",
    "        if not os.path.exists(os.path.join(output_path, source, subdir)):\n",
    "            os.makedirs(os.path.join(output_path, source, subdir), exist_ok=True)\n",
    "        questions = collections.defaultdict(list)\n",
    "        for file in tqdm(os.listdir(os.path.join(root_path_raw, source, subdir)), desc=f\"Processing {source}/{subdir}\"):\n",
    "            with open(os.path.join(root_path_raw, source, subdir, file), \"r\") as f:\n",
    "                d = json.load(f)\n",
    "                try:\n",
    "                    exam = Exam(**d)\n",
    "                    logging.info(f\"Successfully parsed {file}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to parse {os.path.join(source, subdir, file)}: {e}\")\n",
    "                    continue\n",
    "                for question_set in exam.question_sets:\n",
    "                    questions[question_set.type].append(question_set)\n",
    "        for question_type, question_set in questions.items():\n",
    "            gather_exam = Exam(question_sets=question_set)\n",
    "            with open(os.path.join(output_path, source, subdir, f\"{question_type}.json\"), \"w\") as f:\n",
    "                f.write(gather_exam.model_dump_json(indent=4))\n",
    "                logging.info(f\"Successfully wrote [{len(question_set)}] questions of [{question_type}] to {os.path.join(source, subdir, f'{question_type}.json')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Multiple Choice Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_output_path = os.path.join(root_path, \"filtered\")\n",
    "if not os.path.exists(filter_output_path):\n",
    "    os.makedirs(filter_output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "import re\n",
    "import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import hashlib\n",
    "\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[\\(\\（][^()\\（\\）\\u4e00-\\u9fff]*[\\u4e00-\\u9fff]+[^()\\（\\）]*[\\)\\）]', '', text)\n",
    "    text = re.sub(r\"__+\", \"<blank>\", text)\n",
    "    tokens = word_tokenize(text)  # Tokenize into words\n",
    "    return tokens\n",
    "\n",
    "def HashTokens(tokens: List[str]) -> str:\n",
    "    return hashlib.md5(\" \".join(tokens).encode()).hexdigest()\n",
    "\n",
    "def HashText(text: str) -> str:\n",
    "    tokens = normalize_text(text)\n",
    "    return HashTokens(tokens)\n",
    "\n",
    "def HashMCQ(question_set: QuestionSet) -> str:\n",
    "    text = question_set.questions[0].text\n",
    "    options = [question_set.questions[0].choices.A, question_set.questions[0].choices.B, question_set.questions[0].choices.C, question_set.questions[0].choices.D]\n",
    "    options.sort()\n",
    "    return HashText(text + \" \".join(options))\n",
    "\n",
    "def HasConsecutiveChineseCharacters(text: str, n_max: int) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the text contains more than n_max consecutive Chinese characters.\n",
    "    \"\"\"\n",
    "    pattern = r'[\\u4e00-\\u9fff]{' + str(n_max) + r',}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return len(matches) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitMCQQuestionSet(question_sets: List[QuestionSet]):\n",
    "    splitted_question_set = []\n",
    "    for question_set in question_sets:\n",
    "        if question_set.type == QuestionType.MCQ:\n",
    "            if question_set.context != \"\":\n",
    "                logging.warning(f\"MCQ Question set has context: {question_set.context}. Will be removed!\")\n",
    "            for question in question_set.questions:\n",
    "                atomic_question_set = QuestionSet(\n",
    "                    type=QuestionType.MCQ,\n",
    "                    context=\"\",\n",
    "                    questions=[question]\n",
    "                )\n",
    "                splitted_question_set.append(atomic_question_set)\n",
    "    return splitted_question_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DealWithBlank(question_set: QuestionSet):\n",
    "    wrong_pattern_and_correction = {\n",
    "        r\"__+\": \"<blank>\",\n",
    "        r\"(\\w)<blank>(\\w)\": r\"\\1 <blank> \\2\",\n",
    "        r\"^<blank>(\\w)\": r\"<blank> \\1\",\n",
    "        r\"(\\w)<blank>$\": r\"\\1 <blank>\",\n",
    "        r\"！\": \"!\",\n",
    "        r\"“\": \"\\\"\",\n",
    "        r\"：\": \":\",\n",
    "        r\"；\": \";\",\n",
    "        r\"？\": \"?\",\n",
    "        r\"（\": \"(\",\n",
    "        r\"）\": \")\",\n",
    "        r\"，\": \",\",\n",
    "        r\"。\": \".\",\n",
    "        r\"、\": \",\",\n",
    "        r\"——\": \"-\",\n",
    "        r\"---+\": \"--\",\n",
    "        # deal with spaces\n",
    "        r\"[ \\t]+\": \" \",\n",
    "        r\"\\s+([.,!?;:])\": r\"\\1\",\n",
    "    }\n",
    "    for question in question_set.questions:\n",
    "        for pattern, correction in wrong_pattern_and_correction.items():\n",
    "            question.text = re.sub(pattern, correction, question.text)\n",
    "            question.choices.A = re.sub(pattern, correction, question.choices.A)\n",
    "            question.choices.B = re.sub(pattern, correction, question.choices.B)\n",
    "            question.choices.C = re.sub(pattern, correction, question.choices.C)\n",
    "            question.choices.D = re.sub(pattern, correction, question.choices.D)\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsQuestionSetWithValidChoices(question_set: QuestionSet):\n",
    "    if len(question_set.questions) == 0:\n",
    "        return False\n",
    "    return question_set.questions[0].choices.A != \"\" and question_set.questions[0].choices.B != \"\" and question_set.questions[0].choices.C != \"\"\n",
    "\n",
    "\n",
    "def IsQuestionTextWithFewChineseCharacters(question_set: QuestionSet):\n",
    "    if len(question_set.questions) == 0:\n",
    "        return False\n",
    "    question = question_set.questions[0].text\n",
    "    if len(re.findall(r\"[\\u4e00-\\u9fff]\", question)) > 6:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessMCQByCriteria(question_sets: List[QuestionSet], criteria_arr: List[Any]):\n",
    "    filtered_question_sets = SplitMCQQuestionSet(question_sets)\n",
    "    for question_set in filtered_question_sets:\n",
    "        DealWithBlank(question_set)\n",
    "    for criteria in criteria_arr:\n",
    "        logging.info(f\"Before filtering: {len(filtered_question_sets)}\")\n",
    "        filtered_question_sets = [question_set for question_set in filtered_question_sets if criteria(question_set)]\n",
    "        logging.info(f\"After filtering: {len(filtered_question_sets)}\")\n",
    "    return filtered_question_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_arr = [\n",
    "    IsQuestionTextWithFewChineseCharacters,\n",
    "    IsQuestionSetWithValidChoices,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = os.listdir(output_path)\n",
    "sources = [source for source in sources if os.path.isdir(os.path.join(output_path, source))]\n",
    "filtered_question_sets = collections.defaultdict(list)\n",
    "test_points = set()\n",
    "\n",
    "\n",
    "for source in sources:\n",
    "    for subdir in os.listdir(os.path.join(output_path, source)):\n",
    "        if not os.path.exists(os.path.join(filtered_output_path, source, subdir)):\n",
    "            os.makedirs(os.path.join(filtered_output_path, source, subdir), exist_ok=True)\n",
    "        for file in os.listdir(os.path.join(output_path, source, subdir)):\n",
    "            if os.path.splitext(file)[1] != \".json\":\n",
    "                continue\n",
    "            for file in os.listdir(os.path.join(output_path, source, subdir)):\n",
    "                if os.path.splitext(file)[1] != \".json\":\n",
    "                    continue\n",
    "                with open(os.path.join(output_path, source, subdir, file), \"r\") as f:\n",
    "                    exam = Exam.model_validate(json.load(f))\n",
    "                    if exam.question_sets[0].type == QuestionType.MCQ:\n",
    "                        filtered_question_sets[os.path.join(source, subdir)] = ProcessMCQByCriteria(exam.question_sets, criteria_arr)\n",
    "                        logging.info(f\"Successfully parsed {file}: Got [{len(filtered_question_sets[os.path.join(source, subdir)])}] questions\")\n",
    "                        for question_set in filtered_question_sets[os.path.join(source, subdir)]:\n",
    "                            new_test_points = question_set.questions[0].test_point.split(\";\")\n",
    "                            for test_point in new_test_points:\n",
    "                                test_points.add(test_point.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_md5 = set()\n",
    "# visited_md5 = {}\n",
    "\n",
    "remove_repeated_question_sets = collections.defaultdict(list)\n",
    "for source, question_sets in filtered_question_sets.items():\n",
    "    logging.info(f\"Processing {source}: Got [{len(question_sets)}] questions\")\n",
    "    for question_set in question_sets:\n",
    "        md5 = HashMCQ(question_set)\n",
    "        if md5 not in visited_md5:\n",
    "            visited_md5.add(md5)\n",
    "            # visited_md5[md5] = question_set\n",
    "            remove_repeated_question_sets[source].append(question_set)\n",
    "        # else:\n",
    "            # logging.info(f\"Repeated question: {question_set.questions[0].text}: {question_set.questions[0].choices.A} {question_set.questions[0].choices.B} {question_set.questions[0].choices.C} {question_set.questions[0].choices.D}\")\n",
    "            # logging.info(f\"Repeated question: {visited_md5[md5].questions[0].text}: {visited_md5[md5].questions[0].choices.A} {visited_md5[md5].questions[0].choices.B} {visited_md5[md5].questions[0].choices.C} {visited_md5[md5].questions[0].choices.D}\")\n",
    "    logging.info(f\"After removing repeated questions: Got [{len(remove_repeated_question_sets[source])}] questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, question_sets in remove_repeated_question_sets.items():\n",
    "    with open(os.path.join(filtered_output_path, key, str(QuestionType.MCQ) + \".json\"), \"w\") as f:\n",
    "        f.write(Exam(question_sets=question_sets).model_dump_json(indent=4))\n",
    "        logging.info(f\"Successfully wrote [{len(question_sets)}] questions to {os.path.join(key, 'MCQ.json')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mcq = sum([len(question_sets) for question_sets in remove_repeated_question_sets.values()])\n",
    "print(f\"total mcq: {total_mcq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(filtered_output_path, \"mcq_test_points.json\"), \"w\") as f:\n",
    "    json.dump(list(test_points), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter MC Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsMCReading(question_set: QuestionSet):\n",
    "    if len(question_set.questions) == 0:\n",
    "        return False\n",
    "    if question_set.type != QuestionType.MC_READING:\n",
    "        return False\n",
    "    if len(normalize_text(question_set.context)) < 50:\n",
    "        return False\n",
    "    for question in question_set.questions:\n",
    "        if len(question.text) == 0:\n",
    "            return False\n",
    "        if question.choices.A == \"\" or question.choices.B == \"\" or question.choices.C == \"\":\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def HashMCReading(question_set: QuestionSet) -> str:\n",
    "    text = question_set.context\n",
    "    questions = sorted([question.text for question in question_set.questions])\n",
    "    return HashText(text + \" \".join(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_md5 = set()\n",
    "mc_reading = collections.defaultdict(list)\n",
    "test_points = set()\n",
    "\n",
    "for source in sources:\n",
    "    for subdir in os.listdir(os.path.join(output_path, source)):\n",
    "        if not os.path.exists(os.path.join(filtered_output_path, source, subdir)):\n",
    "            os.makedirs(os.path.join(filtered_output_path, source, subdir), exist_ok=True)\n",
    "        for file in os.listdir(os.path.join(output_path, source, subdir)):\n",
    "            if os.path.splitext(file)[1] != \".json\":\n",
    "                continue\n",
    "            with open(os.path.join(output_path, source, subdir, file), \"r\") as f:\n",
    "                exam = Exam.model_validate(json.load(f))\n",
    "            for question_set in exam.question_sets:\n",
    "                if IsMCReading(question_set):\n",
    "                    md5 = HashMCReading(question_set)\n",
    "                    if md5 not in visited_md5:\n",
    "                        visited_md5.add(md5)\n",
    "                        mc_reading[os.path.join(source, subdir)].append(question_set)\n",
    "                    for question in question_set.questions:\n",
    "                        new_test_points = question.test_point.split(\";\")\n",
    "                        for test_point in new_test_points:\n",
    "                            test_points.add(test_point.strip())\n",
    "        logging.info(f\"After removing repeated questions: Got [{len(mc_reading[os.path.join(source, subdir)])}] MCReading questions\")\n",
    "\n",
    "with open(os.path.join(filtered_output_path, \"mcreading_test_points.json\"), \"w\") as f:\n",
    "    json.dump(list(test_points), f, indent=4)\n",
    "\n",
    "for key, question_sets in mc_reading.items():\n",
    "    with open(os.path.join(filtered_output_path, key, str(QuestionType.MC_READING) + \".json\"), \"w\") as f:\n",
    "        f.write(Exam(question_sets=question_sets).model_dump_json(indent=4))\n",
    "        logging.info(f\"Successfully wrote [{len(question_sets)}] questions to {os.path.join(key, str(QuestionType.MC_READING) + '.json')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter TF Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsTFReading(question_set: QuestionSet):\n",
    "    if len(question_set.questions) == 0:\n",
    "        return False\n",
    "    if question_set.type != QuestionType.TF_READING:\n",
    "        return False\n",
    "    if len(normalize_text(question_set.context)) < 50:\n",
    "        return False\n",
    "    if HasConsecutiveChineseCharacters(question_set.context, 6):\n",
    "        return False\n",
    "    for question in question_set.questions:\n",
    "        if len(question.text) == 0:\n",
    "            return False\n",
    "        if question.choices.A != \"\" or question.choices.B != \"\" or question.choices.C != \"\" or question.choices.D != \"\":\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def HashTFReading(question_set: QuestionSet) -> str:\n",
    "    text = question_set.context\n",
    "    questions = sorted([question.text for question in question_set.questions])\n",
    "    return HashText(text + \" \".join(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_md5 = set()\n",
    "tf_reading = collections.defaultdict(list)\n",
    "test_points = set()\n",
    "\n",
    "for source in sources:\n",
    "    for subdir in os.listdir(os.path.join(output_path, source)):\n",
    "        if not os.path.exists(os.path.join(filtered_output_path, source, subdir)):\n",
    "            os.makedirs(os.path.join(filtered_output_path, source, subdir), exist_ok=True)\n",
    "        for file in os.listdir(os.path.join(output_path, source, subdir)):\n",
    "            if os.path.splitext(file)[1] != \".json\":\n",
    "                continue\n",
    "            with open(os.path.join(output_path, source, subdir, file), \"r\") as f:\n",
    "                exam = Exam.model_validate(json.load(f))\n",
    "            for question_set in exam.question_sets:\n",
    "                if IsTFReading(question_set):\n",
    "                    md5 = HashTFReading(question_set)\n",
    "                    if md5 not in visited_md5:\n",
    "                        visited_md5.add(md5)\n",
    "                        tf_reading[os.path.join(source, subdir)].append(question_set)\n",
    "                    for question in question_set.questions:\n",
    "                        new_test_points = question.test_point.split(\";\")\n",
    "                        for test_point in new_test_points:\n",
    "                            test_points.add(test_point.strip())\n",
    "        logging.info(f\"After removing repeated questions: Got [{len(tf_reading[os.path.join(source, subdir)])}] TFReading questions\")\n",
    "\n",
    "with open(os.path.join(filtered_output_path, \"tfreading_test_points.json\"), \"w\") as f:\n",
    "    json.dump(list(test_points), f, indent=4)\n",
    "\n",
    "for key, question_sets in tf_reading.items():\n",
    "    with open(os.path.join(filtered_output_path, key, str(QuestionType.TF_READING) + \".json\"), \"w\") as f:\n",
    "        f.write(Exam(question_sets=question_sets).model_dump_json(indent=4))\n",
    "        logging.info(f\"Successfully wrote [{len(question_sets)}] questions to {os.path.join(key, str(QuestionType.TF_READING) + '.json')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter MC Cloze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsMCCloze(question_set: QuestionSet):\n",
    "    if len(question_set.questions) == 0:\n",
    "        return False\n",
    "    if question_set.type != QuestionType.MC_CLOZE:\n",
    "        return False\n",
    "    if len(normalize_text(question_set.context)) < 50:\n",
    "        return False\n",
    "    if HasConsecutiveChineseCharacters(question_set.context, 6):\n",
    "        return False\n",
    "    if \"<blank\" not in question_set.context:\n",
    "        return False\n",
    "    for question in question_set.questions:\n",
    "        if len(question.text) > 0:\n",
    "            return False\n",
    "        if question.choices.A == \"\" or question.choices.B == \"\" or question.choices.C == \"\":\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def HashMCCloze(question_set: QuestionSet) -> str:\n",
    "    text = question_set.context\n",
    "    options = []\n",
    "    for question in question_set.questions:\n",
    "        options.extend(sorted([question.choices.A, question.choices.B, question.choices.C, question.choices.D]))\n",
    "    return HashText(text + \" \".join(options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegularizeMCClozeFormat(question_set: QuestionSet):\n",
    "    if question_set.type != QuestionType.MC_CLOZE:\n",
    "        return\n",
    "    question_set.context = re.sub(r\"……\", \"...\", question_set.context)\n",
    "    question_set.context = re.sub(r\"__+\", \"_\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank>(\\d+)<blank>\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"(\\d+)<blank>\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank>(\\d+)\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"_+<blank\", r\"<blank\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank>_+\", r\"<blank>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank text=(.*)>_+\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank text=\\((.*)\\)>\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"_\", \"\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank \\((\\d+)\\)>\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank (\\d+)>\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank>\\((\\d+)\\)<blank>\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank>\\((\\d+)\\)\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"\\((\\d+)\\)<blank>\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"\\((\\d+)\\)\", r\"<blank text=\\1>\", question_set.context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_md5 = set()\n",
    "mc_clozes = collections.defaultdict(list)\n",
    "test_points = set()\n",
    "\n",
    "for source in sources:\n",
    "    for subdir in os.listdir(os.path.join(output_path, source)):\n",
    "        if not os.path.exists(os.path.join(filtered_output_path, source, subdir)):\n",
    "            os.makedirs(os.path.join(filtered_output_path, source, subdir), exist_ok=True)\n",
    "        for file in os.listdir(os.path.join(output_path, source, subdir)):\n",
    "            if os.path.splitext(file)[1] != \".json\":\n",
    "                continue\n",
    "            with open(os.path.join(output_path, source, subdir, file), \"r\") as f:\n",
    "                exam = Exam.model_validate(json.load(f))\n",
    "            for question_set in exam.question_sets:\n",
    "                if IsMCCloze(question_set):\n",
    "                    RegularizeMCClozeFormat(question_set)\n",
    "                    md5 = HashMCCloze(question_set)\n",
    "                    if md5 not in visited_md5:\n",
    "                        visited_md5.add(md5)\n",
    "                        mc_clozes[os.path.join(source, subdir)].append(question_set)\n",
    "                    for question in question_set.questions:\n",
    "                        new_test_points = question.test_point.split(\";\")\n",
    "                        for test_point in new_test_points:\n",
    "                            test_points.add(test_point.strip())\n",
    "        logging.info(f\"After removing repeated questions: Got [{len(mc_clozes[os.path.join(source, subdir)])}] MC Cloze questions\")\n",
    "\n",
    "with open(os.path.join(filtered_output_path, \"mc_cloze_test_points.json\"), \"w\") as f:\n",
    "    json.dump(list(test_points), f, indent=4)\n",
    "\n",
    "for key, question_sets in mc_clozes.items():\n",
    "    with open(os.path.join(filtered_output_path, key, str(QuestionType.MC_CLOZE) + \".json\"), \"w\") as f:\n",
    "        f.write(Exam(question_sets=question_sets).model_dump_json(indent=4))\n",
    "        logging.info(f\"Successfully wrote [{len(question_sets)}] questions to {os.path.join(key, str(QuestionType.MC_CLOZE) + '.json')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter FR Cloze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsFRCloze(question_set: QuestionSet):\n",
    "    if len(question_set.questions) == 0:\n",
    "        return False\n",
    "    if question_set.type != QuestionType.FR_CLOZE:\n",
    "        return False\n",
    "    if len(normalize_text(question_set.context)) < 50:\n",
    "        return False\n",
    "    if question_set.context.count(\"<blank\") < 5 or question_set.context.count(\"<blank\") > 10:\n",
    "        return False\n",
    "    if question_set.context.count(\"\\n\") >= 4: \n",
    "        return False\n",
    "    if question_set.context.count(\"<blank\") - question_set.context.count(\"\\n\") < 2:\n",
    "        return False\n",
    "    if HasConsecutiveChineseCharacters(question_set.context, 6):\n",
    "        return False\n",
    "    if len(re.findall(r\"(\\d+)\\.\", question_set.context)) > 3:\n",
    "        return False\n",
    "    for question in question_set.questions:\n",
    "        if len(question.text) > 0:\n",
    "            return False\n",
    "        if question.choices.A != \"\" or question.choices.B != \"\" or question.choices.C != \"\" or question.choices.D != \"\":\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def HashFRCloze(question_set: QuestionSet) -> str:\n",
    "    text = question_set.context\n",
    "    return HashText(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegularizeFRFormat(question_set: QuestionSet):\n",
    "    if question_set.type != QuestionType.FR_CLOZE:\n",
    "        return\n",
    "    question_set.context = re.sub(r\"……\", \"...\", question_set.context)\n",
    "    question_set.context = re.sub(r\"__+\", \"_\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank>(\\d+)<blank>\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank>\\((\\d+)\\)<blank>\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"\\((\\d+)\\)<blank>\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"\\((\\d+)\\) <blank>\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"_+<blank\", r\"<blank\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank>_+\", r\"<blank>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank text=\\((.*)\\)>\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank text=(.*)>_+\", r\"<blank text=\\1>\", question_set.context)\n",
    "    question_set.context = re.sub(r\"<blank text=([^>]+)>\", lambda m: m.group(0) if m.group(1).isdigit() else \"<blank>\", question_set.context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_md5 = set()\n",
    "fr_clozes = collections.defaultdict(list)\n",
    "test_points = set()\n",
    "\n",
    "for source in sources:\n",
    "    for subdir in os.listdir(os.path.join(output_path, source)):\n",
    "        if not os.path.exists(os.path.join(filtered_output_path, source, subdir)):\n",
    "            os.makedirs(os.path.join(filtered_output_path, source, subdir), exist_ok=True)\n",
    "        for file in os.listdir(os.path.join(output_path, source, subdir)):\n",
    "            if os.path.splitext(file)[1] != \".json\":\n",
    "                continue\n",
    "            with open(os.path.join(output_path, source, subdir, file), \"r\") as f:\n",
    "                exam = Exam.model_validate(json.load(f))\n",
    "            for question_set in exam.question_sets:\n",
    "                if IsFRCloze(question_set):\n",
    "                    RegularizeFRFormat(question_set)\n",
    "                    md5 = HashFRCloze(question_set)\n",
    "                    if md5 not in visited_md5:\n",
    "                        visited_md5.add(md5)\n",
    "                        fr_clozes[os.path.join(source, subdir)].append(question_set)\n",
    "                    for question in question_set.questions:\n",
    "                        new_test_points = question.test_point.split(\";\")\n",
    "                        for test_point in new_test_points:\n",
    "                            test_points.add(test_point.strip())\n",
    "        logging.info(f\"After removing repeated questions: Got [{len(fr_clozes[os.path.join(source, subdir)])}] FR Cloze questions\")\n",
    "\n",
    "with open(os.path.join(filtered_output_path, \"fr_cloze_test_points.json\"), \"w\") as f:\n",
    "    json.dump(list(test_points), f, indent=4)\n",
    "\n",
    "for key, question_sets in fr_clozes.items():\n",
    "    with open(os.path.join(filtered_output_path, key, str(QuestionType.FR_CLOZE) + \".json\"), \"w\") as f:\n",
    "        f.write(Exam(question_sets=question_sets).model_dump_json(indent=4))\n",
    "        logging.info(f\"Successfully wrote [{len(question_sets)}] questions to {os.path.join(key, str(QuestionType.FR_CLOZE) + '.json')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs553",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
